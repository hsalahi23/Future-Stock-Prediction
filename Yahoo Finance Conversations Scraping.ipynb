{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6aa919c",
   "metadata": {},
   "source": [
    "# **Dinnect Data Science Hackathon**\n",
    "# Yahoo Finance Conversation Scrapper\n",
    "## Written by: Hamidreza Salahi\n",
    "### 16 Oct 2024\n",
    "## Email: salahi92.h@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag\n",
    "import attr\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import urllib.request\n",
    "import csv\n",
    "from autocorrect import Speller\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3467b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"ONCO\", \"CNEY\", \"TNXP\", \"APLD\", \"KTTA\"]\n",
    "end_date = datetime.strptime('2024-09-26', '%Y-%m-%d').date()\n",
    "latest_start_date = datetime.strptime('2022-04-13', '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime by adding time (00:00:00)\n",
    "latest_start_datetime = datetime.combine(latest_start_date, datetime.min.time())\n",
    "earliest_start_datetime = datetime.combine(end_date, datetime.min.time())\n",
    "# Get the Unix timestamp\n",
    "cutoff_time_epoch = latest_start_datetime.timestamp()\n",
    "max_epoch_time = earliest_start_datetime.timestamp()\n",
    "\n",
    "# Function to scrape comments for a single ticker\n",
    "def scrape_comments_for_ticker(ticker, msg_count_per_page=60, cutoff_time_epoch=cutoff_time_epoch, max_epoch_time = max_epoch_time ):\n",
    "    # Initialize the list to store parsed data\n",
    "    parsed_data = []\n",
    "    \n",
    "    # Yahoo Finance community page for the ticker\n",
    "    url = f'https://finance.yahoo.com/quote/{ticker}/community?p={ticker}'\n",
    "    \n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    data = json.loads(soup.select_one('#spotim-config').get_text(strip=True))['config']\n",
    "    \n",
    "    \n",
    "    api_url = \"https://api-2-0.spot.im/v1.0.0/conversation/read\"\n",
    "\n",
    "    # Prepare the payload and headers for the API request\n",
    "    payload = json.dumps({\n",
    "        \"conversation_id\": data['spotId'] + data['uuid'].replace('_', '$'),\n",
    "        \"count\": msg_count_per_page,\n",
    "        \"offset\": 0\n",
    "    })\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0',\n",
    "        'Content-Type': 'application/json',\n",
    "        'x-spot-id': data['spotId'],\n",
    "        'x-post-id': data['uuid'].replace('_', '$'),\n",
    "    }\n",
    "    time.sleep(2)\n",
    "    # Make the first request to get the total number of messages\n",
    "    response = requests.post(api_url, headers=headers, data=payload)\n",
    "    data = response.json()\n",
    "    total_num_msgs = data['conversation']['messages_count']\n",
    "    max_offsets = total_num_msgs\n",
    "    time.sleep(5)\n",
    "    # Iterate over offsets in chunks to get all messages\n",
    "    offsets = range(0, max_offsets, msg_count_per_page)\n",
    "\n",
    "    for offset in offsets:\n",
    "        try:\n",
    "            url = f'https://finance.yahoo.com/quote/{ticker}/community?p={ticker}'\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0'})\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            data = json.loads(soup.select_one('#spotim-config').get_text(strip=True))['config']\n",
    "            url = \"https://api-2-0.spot.im/v1.0.0/conversation/read\"\n",
    "\n",
    "            payload = json.dumps({\n",
    "              \"conversation_id\": data['spotId'] + data['uuid'].replace('_', '$'),\n",
    "              \"count\": msg_count_per_page,\n",
    "              \"offset\": offset\n",
    "            })\n",
    "            headers = {\n",
    "              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0',\n",
    "              'Content-Type': 'application/json',\n",
    "              'x-spot-id': data['spotId'],\n",
    "              'x-post-id': data['uuid'].replace('_', '$'),\n",
    "            }\n",
    "            time.sleep(2)\n",
    "            response = requests.post(url, headers=headers, data=payload)\n",
    "            data = response.json()\n",
    "            comments = data['conversation']['comments']\n",
    "            for comment in comments:\n",
    "                content_texts = comment['content']\n",
    "                for content in content_texts:\n",
    "                    if content['type']=='text':\n",
    "                        comment_time_epoch = comment['written_at']\n",
    "                        if comment_time_epoch > max_epoch_time:\n",
    "                            continue\n",
    "                        text = content['text']\n",
    "                        parsed_data.append([text, comment_time_epoch])\n",
    "                replies_count = comment['replies_count']\n",
    "                if replies_count!=0:\n",
    "                    replies = comment['replies']\n",
    "                    for reply in replies:\n",
    "                        reply_time = reply['written_at']\n",
    "                        reply_content = reply['content']\n",
    "                        for reply in reply_content:\n",
    "                            if reply['type']=='text':\n",
    "                                reply_text = reply['text']\n",
    "                                parsed_data.append([reply_text, reply_time])\n",
    "            if comment_time_epoch < cutoff_time_epoch:\n",
    "                break\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping offset {offset} for ticker {ticker}: {e}\")\n",
    "            time.sleep(5)\n",
    "    return parsed_data\n",
    "\n",
    "# Scrape data for all tickers\n",
    "all_tickers_parsed_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Scraping comments for {ticker}...\")\n",
    "    ticker_data = scrape_comments_for_ticker(ticker)\n",
    "    all_tickers_parsed_data[ticker] = (ticker_data)\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_tickers_parsed_data.json', 'w') as json_file:\n",
    "    json.dump(all_tickers_parsed_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0a9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "with open('all_tickers_parsed_data.json', 'r') as json_file:\n",
    "    all_tickers_parsed_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a1385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickers_comments_df = {}\n",
    "\n",
    "for ticker, ticker_comments in all_tickers_parsed_data.items():\n",
    "    comment_df = pd.DataFrame(ticker_comments, columns=['text','date'])\n",
    "    comment_df['date'] = pd.to_datetime(comment_df['date'], unit='s', utc=True).dt.tz_convert('America/New_York')\n",
    "    comment_df['date'] = pd.to_datetime(comment_df['date']).dt.date\n",
    "    all_tickers_comments_df[ticker] = comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38e7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Initialization the twitter tokenizer\n",
    "    tk = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True) \n",
    "    # Initialization the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    # Trying to avoid deleting the negative verbs as it affects the meaning of the comments.\n",
    "    stop_words = stopwords.words('english') + [\"i'll\",\"i'm\", \"should\", \"could\"]\n",
    "    negative_verbs = [ \"shan't\",'shouldn',\"shouldn't\",'wasn','weren','won','wouldn','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn',\"mustn't\",'needn',\"needn't\",\"wouldn't\",\"won't\",\"weren't\",\"wasn't\",\"couldn\",\"not\",\"nor\",\"no\",\"mightn't\",\"isn't\",\"haven't\",\"hadn't\",\"hasn't\",\"didn't\",\"doesn't\",\"aren't\",\"don't\",\"couldn't\",\"never\"]\n",
    "    stop_words =[word for word in stop_words if word not in negative_verbs ] \n",
    "    \n",
    "    # Lowering comments\n",
    "    lower_comment = text.lower() \n",
    "    # Removing hashtag and cashtag symbols\n",
    "    comment = re.sub(r\"[#$]\",\" \",lower_comment)\n",
    "    # Removing links from comments\n",
    "    comment = re.sub(r\"https?:\\/\\/.*[\\r\\n]*\",\" \", comment)\n",
    "    # Translating emojies into thier descriptions\n",
    "    comment = demoji.replace_with_desc(comment)\n",
    "    # removing numerical values\n",
    "    comment = re.sub(r\"[0-9]|-->\",\"\",comment)\n",
    "    comment = re.sub(r\"<.*?>\", \" \", comment)\n",
    "    # Tokenize the comments by twitter tokenzier.\n",
    "    comment = tk.tokenize(comment)\n",
    "    # Choosing the words that don't exist in stopwords, thier lengths are more than 2 letters and then lemmatize them.\n",
    "    comment = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in comment if word not in stop_words and word not in string.punctuation and len(word)>2 and \".\" not in word]\n",
    "    # return the tokens in one sentence \n",
    "    comment = \" \".join(comment)\n",
    "    \n",
    "    return comment\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4510915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker, comment_df in all_tickers_comments_df.items():\n",
    "    comment_df['cleaned'] = comment_df[\"text\"].apply(lambda row:clean_text(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89fe484",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_mapping = {}\n",
    "for ticker, df in all_tickers_comments_df.items():\n",
    "    df.dropna(inplace=True)\n",
    "    file_name = f\"{ticker}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    csv_file_mapping[ticker] = file_name\n",
    "\n",
    "# Save the mapping of ticker names to file paths as a JSON file\n",
    "with open('ticker_file_mapping.json', 'w') as json_file:\n",
    "    json.dump(csv_file_mapping, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297be7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file path mapping\n",
    "with open('ticker_file_mapping.json', 'r') as json_file:\n",
    "    csv_file_mapping = json.load(json_file)\n",
    "\n",
    "# Load each CSV into a DataFrame and recreate the original dictionary\n",
    "all_tickers_comments_df = {}\n",
    "for ticker, file_name in csv_file_mapping.items():\n",
    "    all_tickers_comments_df[ticker] = pd.read_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df91f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()\n",
    "tickers_pol_df = {}\n",
    "for ticker, comments in all_tickers_comments_df.items():\n",
    "    comments['polarity'] = comments.text.apply(lambda s: sent.polarity_scores(s)['compound'])\n",
    "    comments['date'] =pd.to_datetime(comments['date'],infer_datetime_format=True)\n",
    "    comments['date'] =pd.to_datetime(comments['date'].dt.strftime(\"%m/%d/%y\"))\n",
    "    Pol_df = pd.DataFrame(comments.groupby('date')['polarity'].mean())\n",
    "    tickers_pol_df[ticker] = Pol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7a503e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_Pol_mapping = {}\n",
    "for ticker, df in tickers_pol_df.items():\n",
    "    df.dropna(inplace=True)\n",
    "    file_name = f\"{ticker}_Pol.csv\"\n",
    "    df.to_csv(file_name, index=True)\n",
    "    csv_Pol_mapping[ticker] = file_name\n",
    "\n",
    "# Save the mapping of ticker names to file paths as a JSON file\n",
    "with open('ticker_Pol_mapping.json', 'w') as json_file:\n",
    "    json.dump(csv_Pol_mapping, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
